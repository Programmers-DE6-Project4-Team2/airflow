"""
ì˜¬ë¦¬ë¸Œì˜ ë¦¬ë·° ìˆ˜ì§‘ DAG
ë¦¬ë·° í¬ë¡¤ëŸ¬ ë°°í¬ ë° ìˆ˜ì§‘ëœ ìƒí’ˆ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° ìƒí’ˆì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ GCSì— ì €ì¥
"""

import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.models import Variable
import pendulum
from google.cloud import storage

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'de6-team2',
    'depends_on_past': False,
    'start_date': pendulum.now('UTC').subtract(days=1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'max_active_runs': 1,
}

# GCP ì„¤ì •
PROJECT_ID = "de6-2ez"
REGION = "asia-northeast3"
BUCKET_NAME = f"{PROJECT_ID}-raw-data"
CLOUD_RUN_JOB_NAME = "oliveyoung-review-scraper"

def deploy_review_scraper(**context):
    """ë¦¬ë·° í¬ë¡¤ëŸ¬ Cloud Run ì„œë¹„ìŠ¤ ë°°í¬"""
    import subprocess
    
    try:
        logger.info("ğŸ“¦ ë¦¬ë·° í¬ë¡¤ëŸ¬ Cloud Run ì„œë¹„ìŠ¤ ë°°í¬ ì‹œì‘...")
        
        # ì†ŒìŠ¤ ì½”ë“œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        source_dir = "/opt/airflow/scripts/cloud_run/review_scraper"
        docker_repo = f"{REGION}-docker.pkg.dev/{PROJECT_ID}/oliveyoung-scrapers"
        
        # 1. Docker ì´ë¯¸ì§€ ë¹Œë“œ
        logger.info("ğŸ”¨ Docker ì´ë¯¸ì§€ ë¹Œë“œ ì¤‘...")
        build_cmd = [
            'gcloud', 'builds', 'submit',
            '--tag', f'{docker_repo}/review-scraper:latest',
            '--project', PROJECT_ID,
            '--timeout', '20m',
            source_dir
        ]
        
        result = subprocess.run(build_cmd, capture_output=True, text=True, check=True)
        logger.info("âœ… Docker ì´ë¯¸ì§€ ë¹Œë“œ ì™„ë£Œ")
        
        # 2. Cloud Run ì„œë¹„ìŠ¤ ë°°í¬
        logger.info("ğŸš¢ Cloud Run ì„œë¹„ìŠ¤ ë°°í¬ ì¤‘...")
        deploy_cmd = [
            'gcloud', 'run', 'deploy', 'oliveyoung-review-scraper',
            '--image', f'{docker_repo}/review-scraper:latest',
            '--platform', 'managed',
            '--region', REGION,
            '--project', PROJECT_ID,
            '--memory', '4Gi',
            '--cpu', '2',
            '--timeout', '3600',
            '--max-instances', '10',
            '--set-env-vars', f'PROJECT_ID={PROJECT_ID},GCS_BUCKET={PROJECT_ID}-raw-data',
            '--service-account', f'dataproc-serverless-sa@{PROJECT_ID}.iam.gserviceaccount.com',
            '--no-allow-unauthenticated',
            '--quiet'
        ]
        
        result = subprocess.run(deploy_cmd, capture_output=True, text=True, check=True)
        logger.info("âœ… ë¦¬ë·° í¬ë¡¤ëŸ¬ Cloud Run ì„œë¹„ìŠ¤ ë°°í¬ ì™„ë£Œ")
        
        # 3. ì„œë¹„ìŠ¤ URL í™•ì¸
        url_cmd = [
            'gcloud', 'run', 'services', 'describe', 'oliveyoung-review-scraper',
            '--region', REGION,
            '--project', PROJECT_ID,
            '--format', 'value(status.url)'
        ]
        
        result = subprocess.run(url_cmd, capture_output=True, text=True, check=True)
        service_url = result.stdout.strip()
        logger.info(f"ğŸ“Š ì„œë¹„ìŠ¤ URL: {service_url}")
        
        return {
            'status': 'success',
            'service_url': service_url,
            'deployed_at': datetime.now().isoformat()
        }
        
    except subprocess.CalledProcessError as e:
        error_msg = f"ë°°í¬ ì‹¤íŒ¨: {e.stderr if e.stderr else str(e)}"
        logger.error(error_msg)
        raise Exception(error_msg)
    except Exception as e:
        error_msg = f"ë°°í¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        raise Exception(error_msg)

def get_product_data_from_gcs(**context) -> List[Dict]:
    """GCSì—ì„œ ìµœì‹  ìƒí’ˆ ë°ì´í„° ë¡œë“œ"""
    execution_date = context['ds']
    
    try:
        # GCS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = storage.Client(project=PROJECT_ID)
        bucket = client.bucket(BUCKET_NAME)
        
        # ìµœì‹  ìƒí’ˆ ë°ì´í„° íŒŒì¼ë“¤ ì°¾ê¸°
        prefix = "oliveyoung/products/"
        blobs = list(client.list_blobs(bucket, prefix=prefix))
        
        if not blobs:
            raise Exception("No product data found in GCS")
        
        # ìµœì‹  íŒŒì¼ë“¤ë§Œ í•„í„°ë§ (ì˜¤ëŠ˜ ë˜ëŠ” ìµœê·¼ ë°ì´í„°)
        recent_blobs = []
        for blob in blobs:
            if blob.name.endswith('.json'):
                # íŒŒì¼ ê²½ë¡œì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
                if execution_date.replace('-', '') in blob.name or \
                   datetime.now().strftime('%Y%m%d') in blob.name:
                    recent_blobs.append(blob)
        
        if not recent_blobs:
            # ìµœì‹  íŒŒì¼ë“¤ ì¤‘ ê°€ì¥ ìµœê·¼ ê²ƒë“¤ ì„ íƒ
            recent_blobs = sorted(blobs, key=lambda x: x.time_created, reverse=True)[:10]
        
        logger.info(f"Found {len(recent_blobs)} recent product data files")
        
        # ëª¨ë“  ìƒí’ˆ ë°ì´í„° ìˆ˜ì§‘
        all_products = []
        
        for blob in recent_blobs:
            try:
                json_data = blob.download_as_text()
                data = json.loads(json_data)
                
                products = data.get('products', [])
                for product in products:
                    if product.get('url') and product.get('product_id'):
                        all_products.append({
                            'product_id': product.get('product_id'),
                            'product_name': product.get('name', ''),
                            'product_url': product.get('url'),
                            'category_name': product.get('category_name', ''),
                            'brand': product.get('brand', ''),
                            'price': product.get('price', ''),
                            'rating': product.get('rating', ''),
                            'review_count': product.get('review_count', ''),
                        })
                
                logger.info(f"Loaded {len(products)} products from {blob.name}")
                
            except Exception as e:
                logger.error(f"Error loading data from {blob.name}: {str(e)}")
                continue
        
        # ì¤‘ë³µ ì œê±° (product_id ê¸°ì¤€)
        unique_products = {}
        for product in all_products:
            product_id = product['product_id']
            if product_id and product_id not in unique_products:
                unique_products[product_id] = product
        
        final_products = list(unique_products.values())
        logger.info(f"Total unique products for review scraping: {len(final_products)}")
        
        # ìƒí’ˆ ìˆ˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì œí•œ (ë¦¬ì†ŒìŠ¤ ê³ ë ¤)
        max_products = 100  # í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ ìƒí’ˆ ìˆ˜
        if len(final_products) > max_products:
            final_products = final_products[:max_products]
            logger.info(f"Limited to {max_products} products for this execution")
        
        return final_products
        
    except Exception as e:
        logger.error(f"Error loading product data from GCS: {str(e)}")
        raise

def create_review_scraping_batches(products: List[Dict], batch_size: int = 10) -> List[List[Dict]]:
    """ìƒí’ˆë“¤ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°"""
    batches = []
    for i in range(0, len(products), batch_size):
        batch = products[i:i + batch_size]
        batches.append(batch)
    
    logger.info(f"Created {len(batches)} batches with {batch_size} products each")
    return batches

def execute_review_scraping_batch(batch: List[Dict], batch_number: int, **context):
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¦¬ë·° í¬ë¡¤ë§ ì‹¤í–‰"""
    import requests
    
    # Cloud Run ì„œë¹„ìŠ¤ URL (ì‹¤ì œ ë°°í¬ í›„ ìˆ˜ì • í•„ìš”)
    cloud_run_url = f"https://{CLOUD_RUN_JOB_NAME}-{REGION}.run.app"
    
    batch_results = []
    successful_scrapes = 0
    failed_scrapes = 0
    
    logger.info(f"Starting batch {batch_number} with {len(batch)} products")
    
    for product in batch:
        try:
            # ë¦¬ë·° í¬ë¡¤ë§ ìš”ì²­ ë°ì´í„°
            scraping_data = {
                'product_id': product['product_id'],
                'product_url': product['product_url'],
                'product_name': product['product_name'],
                'category_name': product['category_name'],
                'max_reviews': 50,  # ìƒí’ˆë‹¹ ìµœëŒ€ ë¦¬ë·° ìˆ˜
            }
            
            logger.info(f"Scraping reviews for: {product['product_name']} (ID: {product['product_id']})")
            
            # Cloud Run ì„œë¹„ìŠ¤ í˜¸ì¶œ
            response = requests.post(
                f"{cloud_run_url}/scrape",
                json=scraping_data,
                timeout=1800,  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
                headers={'Content-Type': 'application/json'}
            )
            
            if response.status_code == 200:
                result = response.json()
                result['batch_number'] = batch_number
                batch_results.append(result)
                successful_scrapes += 1
                
                reviews_count = result.get('reviews_count', 0)
                logger.info(f"âœ… {product['product_name']}: {reviews_count} reviews")
                
            else:
                error_msg = f"Failed to scrape reviews for {product['product_name']}: {response.status_code}"
                logger.error(error_msg)
                failed_scrapes += 1
                
                batch_results.append({
                    'status': 'error',
                    'product_id': product['product_id'],
                    'product_name': product['product_name'],
                    'error': error_msg,
                    'batch_number': batch_number
                })
                
        except Exception as e:
            error_msg = f"Exception scraping {product['product_name']}: {str(e)}"
            logger.error(error_msg)
            failed_scrapes += 1
            
            batch_results.append({
                'status': 'error',
                'product_id': product['product_id'],
                'product_name': product['product_name'],
                'error': error_msg,
                'batch_number': batch_number
            })
    
    batch_summary = {
        'batch_number': batch_number,
        'total_products': len(batch),
        'successful_scrapes': successful_scrapes,
        'failed_scrapes': failed_scrapes,
        'results': batch_results
    }
    
    logger.info(f"Batch {batch_number} completed: {successful_scrapes} success, {failed_scrapes} failed")
    return batch_summary

def validate_review_scraping_results(**context):
    """ë¦¬ë·° í¬ë¡¤ë§ ê²°ê³¼ ê²€ì¦ ë° ìš”ì•½"""
    ti = context['ti']
    
    # ëª¨ë“  ë°°ì¹˜ ê²°ê³¼ ìˆ˜ì§‘
    total_products = 0
    total_reviews = 0
    successful_products = 0
    failed_products = 0
    batch_summaries = []
    
    # ë°°ì¹˜ íƒœìŠ¤í¬ë“¤ì˜ ê²°ê³¼ ìˆ˜ì§‘
    for i in range(10):  # ìµœëŒ€ 10ê°œ ë°°ì¹˜ ê°€ì •
        task_id = f"scrape_reviews_batch_{i}"
        try:
            batch_result = ti.xcom_pull(task_ids=task_id)
            if batch_result:
                batch_summaries.append(batch_result)
                total_products += batch_result.get('total_products', 0)
                successful_products += batch_result.get('successful_scrapes', 0)
                failed_products += batch_result.get('failed_scrapes', 0)
                
                # ê° ë°°ì¹˜ì˜ ë¦¬ë·° ìˆ˜ í•©ê³„
                for result in batch_result.get('results', []):
                    if result.get('status') == 'completed':
                        total_reviews += result.get('reviews_count', 0)
                
        except Exception as e:
            logger.warning(f"Could not get result from {task_id}: {str(e)}")
            continue
    
    # ì „ì²´ ìš”ì•½
    summary = {
        'execution_date': context['ds'],
        'total_batches': len(batch_summaries),
        'total_products_processed': total_products,
        'successful_products': successful_products,
        'failed_products': failed_products,
        'total_reviews_scraped': total_reviews,
        'success_rate': (successful_products / total_products * 100) if total_products > 0 else 0,
        'batch_details': batch_summaries,
        'scraping_completed_at': datetime.now().isoformat()
    }
    
    logger.info("=== ì˜¬ë¦¬ë¸Œì˜ ë¦¬ë·° í¬ë¡¤ë§ ì™„ë£Œ ===")
    logger.info(f"ì´ ìƒí’ˆ: {summary['total_products_processed']}")
    logger.info(f"ì„±ê³µ: {summary['successful_products']}")
    logger.info(f"ì‹¤íŒ¨: {summary['failed_products']}")
    logger.info(f"ì´ ë¦¬ë·° ìˆ˜: {summary['total_reviews_scraped']}")
    logger.info(f"ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
    
    # ì‹¤íŒ¨ìœ¨ì´ ë†’ìœ¼ë©´ ê²½ê³ 
    if summary['success_rate'] < 70:
        logger.warning(f"ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤: {summary['success_rate']:.1f}%")
    
    return summary

# DAG ìƒì„±
dag = DAG(
    'oliveyoung_reviews_collection',
    default_args=default_args,
    description='ì˜¬ë¦¬ë¸Œì˜ ë¦¬ë·° í¬ë¡¤ëŸ¬ ë°°í¬ ë° ë°ì´í„° ìˆ˜ì§‘',
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰ë§Œ
    catchup=False,
    max_active_runs=1,
    tags=['oliveyoung', 'reviews', 'scraping', 'deployment', 'manual'],
)

# Cloud Run ì„œë¹„ìŠ¤ ë°°í¬ íƒœìŠ¤í¬
deploy_service = PythonOperator(
    task_id='deploy_review_scraper_service',
    python_callable=deploy_review_scraper,
    execution_timeout=timedelta(minutes=30),
    dag=dag,
)

# ìƒí’ˆ ë°ì´í„° ë¡œë“œ íƒœìŠ¤í¬
load_product_data = PythonOperator(
    task_id='load_product_data_from_gcs',
    python_callable=get_product_data_from_gcs,
    dag=dag,
)

# ë°°ì¹˜ ìƒì„± íƒœìŠ¤í¬ (ë™ì ìœ¼ë¡œ ìƒì„±)
def create_review_batches(**context):
    """ë¦¬ë·° í¬ë¡¤ë§ ë°°ì¹˜ ìƒì„±"""
    ti = context['ti']
    products = ti.xcom_pull(task_ids='load_product_data_from_gcs')
    
    if not products:
        raise Exception("No products data found")
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë¦¬ì†ŒìŠ¤ì— ë”°ë¼ ì¡°ì •)
    batch_size = 10
    batches = create_review_scraping_batches(products, batch_size)
    
    return batches

create_batches = PythonOperator(
    task_id='create_review_batches',
    python_callable=create_review_batches,
    dag=dag,
)

# ë°°ì¹˜ë³„ ë¦¬ë·° í¬ë¡¤ë§ íƒœìŠ¤í¬ë“¤ (ìµœëŒ€ 10ê°œ ë°°ì¹˜)
def create_batch_tasks():
    """ë°°ì¹˜ë³„ íƒœìŠ¤í¬ ë™ì  ìƒì„±"""
    batch_tasks = []
    
    for i in range(10):  # ìµœëŒ€ 10ê°œ ë°°ì¹˜
        def create_batch_task(batch_idx):
            def execute_batch(**context):
                ti = context['ti']
                batches = ti.xcom_pull(task_ids='create_review_batches')
                
                if not batches or batch_idx >= len(batches):
                    logger.info(f"No batch {batch_idx} to process")
                    return None
                
                batch = batches[batch_idx]
                return execute_review_scraping_batch(batch, batch_idx, **context)
            
            return PythonOperator(
                task_id=f'scrape_reviews_batch_{batch_idx}',
                python_callable=execute_batch,
                dag=dag,
                # pool='review_scraping_pool',  # ë™ì‹œ ì‹¤í–‰ ì œí•œ - Composer í™˜ê²½ì—ì„œ í™•ì¸ í•„ìš”
            )
        
        batch_task = create_batch_task(i)
        batch_tasks.append(batch_task)
    
    return batch_tasks

batch_tasks = create_batch_tasks()

# ê²°ê³¼ ê²€ì¦ íƒœìŠ¤í¬
validate_results = PythonOperator(
    task_id='validate_review_scraping_results',
    python_callable=validate_review_scraping_results,
    dag=dag,
)

# ì™„ë£Œ ì•Œë¦¼ íƒœìŠ¤í¬
completion_notification = BashOperator(
    task_id='completion_notification',
    bash_command='''
    echo "=== ì˜¬ë¦¬ë¸Œì˜ ë¦¬ë·° í¬ë¡¤ë§ ì™„ë£Œ ==="
    echo "ì‹¤í–‰ ë‚ ì§œ: {{ ds }}"
    echo "ì™„ë£Œ ì‹œê°: $(date)"
    echo "ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” GCSì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
    ''',
    dag=dag,
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
# 1. ë¨¼ì € Cloud Run ì„œë¹„ìŠ¤ ë°°í¬
# 2. ë°°í¬ ì™„ë£Œ í›„ ìƒí’ˆ ë°ì´í„° ë¡œë“œ
# 3. ë°°ì¹˜ ìƒì„± ë° ë¦¬ë·° í¬ë¡¤ë§ ë³‘ë ¬ ì‹¤í–‰
# 4. ê²°ê³¼ ê²€ì¦ ë° ì™„ë£Œ ì•Œë¦¼

deploy_service >> load_product_data >> create_batches >> batch_tasks >> validate_results >> completion_notification

# ë°°ì¹˜ íƒœìŠ¤í¬ë“¤ ë³‘ë ¬ ì‹¤í–‰ ì„¤ì •
for task in batch_tasks:
    create_batches >> task