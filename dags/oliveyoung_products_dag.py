"""
ì˜¬ë¦¬ë¸Œì˜ ìƒí’ˆ ìˆ˜ì§‘ DAG
ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìƒí’ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ GCSì— ì €ì¥
"""

import sys
import os
import logging
from datetime import datetime, timedelta
from typing import Dict, List

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.models import Variable
from airflow.utils.dates import days_ago

# Common ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append('/opt/airflow/common')

# Common ëª¨ë“ˆ import
from categories import OLIVEYOUNG_CATEGORIES, create_safe_task_id, get_gcs_path
from cloud_run_client import create_cloud_run_client
from gcs_uploader import create_gcs_uploader
from data_schemas import validate_oliveyoung_data

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'dawit0905@gmail.com',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'max_active_runs': 1,
}

# GCP ì„¤ì •
PROJECT_ID = "de6-2ez"
REGION = "asia-northeast3"
CLOUD_RUN_SERVICE_NAME = "oliveyoung-product-scraper"
GCS_BUCKET_NAME = f"{PROJECT_ID}-raw-data"

def create_scraping_tasks(**context) -> List[Dict]:
    """ê° ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§ ì‘ì—… ìƒì„±"""
    tasks = []
    execution_date = context['ds']
    
    for category_name, category_info in OLIVEYOUNG_CATEGORIES.items():
        task_info = {
            'category_name': category_name,
            'category_url': category_info['url'],
            'execution_date': execution_date,
            'max_pages': category_info.get('max_pages', 5),
        }
        tasks.append(task_info)
    
    logger.info(f"Created {len(tasks)} scraping tasks for {len(OLIVEYOUNG_CATEGORIES)} categories")
    return tasks

def execute_category_scraping(category_name: str, category_url: str, max_pages: int = 5, **context):
    """ê°œë³„ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤í–‰"""
    
    # Cloud Run ì„œë¹„ìŠ¤ URL
    cloud_run_url = f"https://{CLOUD_RUN_SERVICE_NAME}-{REGION}.run.app"
    
    try:
        logger.info(f"ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘: {category_name}")
        logger.info(f"ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        
        # Cloud Run í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = create_cloud_run_client(cloud_run_url)
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        result = client.scrape_category(
            category_name=category_name,
            category_url=category_url,
            max_pages=max_pages
        )
        
        # ê²°ê³¼ í™•ì¸
        if result['status'] == 'success':
            logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {category_name}")
            logger.info(f"ìˆ˜ì§‘ëœ ìƒí’ˆ ìˆ˜: {result.get('products_count', 0)}")
            logger.info(f"GCS ì €ì¥ ê²½ë¡œ: {result.get('gcs_path', '')}")
            return result
        else:
            error_msg = f"í¬ë¡¤ë§ ì‹¤íŒ¨: {category_name} - {result.get('error_message', '')}"
            logger.error(error_msg)
            raise Exception(error_msg)
            
    except Exception as e:
        logger.error(f"ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì˜¤ë¥˜ {category_name}: {str(e)}")
        raise
    finally:
        # í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
        if 'client' in locals():
            client.close()

def deploy_product_scraper(**context):
    """ìƒí’ˆ í¬ë¡¤ëŸ¬ Cloud Run ì„œë¹„ìŠ¤ ë°°í¬"""
    import subprocess
    
    try:
        logger.info("ğŸ“¦ ì˜¬ë¦¬ë¸Œì˜ í¬ë¡¤ëŸ¬ Cloud Run ì„œë¹„ìŠ¤ ë°°í¬ ì‹œì‘...")
        
        # ì†ŒìŠ¤ ì½”ë“œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        source_dir = "/opt/airflow/scripts/cloud_run/oliveyoung_scraper"
        docker_repo = f"{REGION}-docker.pkg.dev/{PROJECT_ID}/oliveyoung-scrapers"
        
        # 1. Docker ì´ë¯¸ì§€ ë¹Œë“œ
        logger.info("ğŸ”¨ Docker ì´ë¯¸ì§€ ë¹Œë“œ ì¤‘...")
        build_cmd = [
            'gcloud', 'builds', 'submit',
            '--tag', f'{docker_repo}/oliveyoung-scraper:latest',
            '--project', PROJECT_ID,
            '--timeout', '20m',
            source_dir
        ]
        
        result = subprocess.run(build_cmd, capture_output=True, text=True, check=True)
        logger.info("âœ… Docker ì´ë¯¸ì§€ ë¹Œë“œ ì™„ë£Œ")
        
        # 2. Cloud Run ì„œë¹„ìŠ¤ ë°°í¬
        logger.info("ğŸš¢ Cloud Run ì„œë¹„ìŠ¤ ë°°í¬ ì¤‘...")
        deploy_cmd = [
            'gcloud', 'run', 'deploy', CLOUD_RUN_SERVICE_NAME,
            '--image', f'{docker_repo}/oliveyoung-scraper:latest',
            '--platform', 'managed',
            '--region', REGION,
            '--project', PROJECT_ID,
            '--memory', '4Gi',
            '--cpu', '2',
            '--timeout', '3600',
            '--max-instances', '10',
            '--set-env-vars', f'PROJECT_ID={PROJECT_ID},GCS_BUCKET={GCS_BUCKET_NAME}',
            '--service-account', f'dataproc-serverless-sa@{PROJECT_ID}.iam.gserviceaccount.com',
            '--allow-unauthenticated',
            '--quiet'
        ]
        
        result = subprocess.run(deploy_cmd, capture_output=True, text=True, check=True)
        logger.info("âœ… ì˜¬ë¦¬ë¸Œì˜ í¬ë¡¤ëŸ¬ Cloud Run ì„œë¹„ìŠ¤ ë°°í¬ ì™„ë£Œ")
        
        # 3. ì„œë¹„ìŠ¤ URL í™•ì¸
        url_cmd = [
            'gcloud', 'run', 'services', 'describe', CLOUD_RUN_SERVICE_NAME,
            '--region', REGION,
            '--project', PROJECT_ID,
            '--format', 'value(status.url)'
        ]
        
        result = subprocess.run(url_cmd, capture_output=True, text=True, check=True)
        service_url = result.stdout.strip()
        logger.info(f"ğŸ“Š ì„œë¹„ìŠ¤ URL: {service_url}")
        
        return {
            'status': 'success',
            'service_url': service_url,
            'service_name': CLOUD_RUN_SERVICE_NAME,
            'deployed_at': datetime.now().isoformat()
        }
        
    except subprocess.CalledProcessError as e:
        error_msg = f"ë°°í¬ ì‹¤íŒ¨: {e.stderr if e.stderr else str(e)}"
        logger.error(error_msg)
        raise Exception(error_msg)
    except Exception as e:
        error_msg = f"ë°°í¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        raise Exception(error_msg)

def validate_scraping_results(**context):
    """í¬ë¡¤ë§ ê²°ê³¼ ê²€ì¦ ë° ìš”ì•½"""
    ti = context['ti']
    
    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì‘ì—… ê²°ê³¼ ìˆ˜ì§‘
    total_products = 0
    successful_categories = []
    failed_categories = []
    
    for category_name in OLIVEYOUNG_CATEGORIES.keys():
        task_id = f"scrape_category_{create_safe_task_id(category_name)}"
        try:
            result = ti.xcom_pull(task_ids=task_id)
            if result and result.get('status') == 'success':
                products_count = result.get('products_count', 0)
                total_products += products_count
                successful_categories.append({
                    'category': category_name,
                    'products_count': products_count,
                    'gcs_path': result.get('gcs_path', ''),
                    'pages_scraped': result.get('pages_scraped', 0),
                    'elapsed_time': result.get('elapsed_time', 0)
                })
                logger.info(f"âœ… {category_name}: {products_count}ê°œ ìƒí’ˆ, {result.get('pages_scraped', 0)}í˜ì´ì§€")
            else:
                failed_categories.append({
                    'category': category_name,
                    'error_message': result.get('error_message', 'Unknown error') if result else 'No result returned'
                })
                logger.error(f"âŒ {category_name}: ì‹¤íŒ¨")
        except Exception as e:
            failed_categories.append({
                'category': category_name,
                'error_message': str(e)
            })
            logger.error(f"âŒ {category_name}: ì˜ˆì™¸ ë°œìƒ - {str(e)}")
    
    # ê²°ê³¼ ìš”ì•½
    summary = {
        'execution_date': context['ds'],
        'total_categories': len(OLIVEYOUNG_CATEGORIES),
        'successful_categories': len(successful_categories),
        'failed_categories': len(failed_categories),
        'total_products_scraped': total_products,
        'success_rate': (len(successful_categories) / len(OLIVEYOUNG_CATEGORIES)) * 100,
        'successful_category_details': successful_categories,
        'failed_category_details': failed_categories,
        'scraping_completed_at': datetime.now().isoformat()
    }
    
    logger.info("=== ì˜¬ë¦¬ë¸Œì˜ ìƒí’ˆ í¬ë¡¤ë§ ì™„ë£Œ ===")
    logger.info(f"ì´ ì¹´í…Œê³ ë¦¬: {summary['total_categories']}ê°œ")
    logger.info(f"ì„±ê³µ: {summary['successful_categories']}ê°œ")
    logger.info(f"ì‹¤íŒ¨: {summary['failed_categories']}ê°œ")
    logger.info(f"ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
    logger.info(f"ì´ ìƒí’ˆ ìˆ˜: {summary['total_products_scraped']}ê°œ")
    
    # ì‹¤íŒ¨í•œ ì¹´í…Œê³ ë¦¬ê°€ ìˆìœ¼ë©´ ê²½ê³ 
    if failed_categories:
        failed_names = [item['category'] for item in failed_categories]
        logger.warning(f"ì‹¤íŒ¨í•œ ì¹´í…Œê³ ë¦¬: {failed_names}")
    
    return summary

# DAG ìƒì„±
dag = DAG(
    'oliveyoung_products_collection',
    default_args=default_args,
    description='ì˜¬ë¦¬ë¸Œì˜ ìƒí’ˆ í¬ë¡¤ëŸ¬ ë°°í¬ ë° ë°ì´í„° ìˆ˜ì§‘',
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰ë§Œ
    catchup=False,
    max_active_runs=1,
    tags=['oliveyoung', 'products', 'scraping', 'deployment', 'manual'],
)

# Cloud Run ì„œë¹„ìŠ¤ ë°°í¬ íƒœìŠ¤í¬
deploy_service = PythonOperator(
    task_id='deploy_product_scraper_service',
    python_callable=deploy_product_scraper,
    execution_timeout=timedelta(minutes=30),
    dag=dag,
)

# ì‘ì—… ìƒì„± íƒœìŠ¤í¬
create_tasks = PythonOperator(
    task_id='create_scraping_tasks',
    python_callable=create_scraping_tasks,
    dag=dag,
)

# ê° ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§ íƒœìŠ¤í¬ ë™ì  ìƒì„±
scraping_tasks = []

for category_name, category_info in OLIVEYOUNG_CATEGORIES.items():
    # íƒœìŠ¤í¬ IDì— ì‚¬ìš©í•  ì•ˆì „í•œ ì´ë¦„ ìƒì„±
    safe_category_name = create_safe_task_id(category_name)
    
    scraping_task = PythonOperator(
        task_id=f'scrape_category_{safe_category_name}',
        python_callable=execute_category_scraping,
        op_kwargs={
            'category_name': category_name,
            'category_url': category_info['url'],
            'max_pages': category_info.get('max_pages', 5),
        },
        dag=dag,
        pool='default_pool',  # ê¸°ë³¸ í’€ ì‚¬ìš©
        execution_timeout=timedelta(minutes=60),  # ê°œë³„ íƒœìŠ¤í¬ íƒ€ì„ì•„ì›ƒ
    )
    
    scraping_tasks.append(scraping_task)

# ê²°ê³¼ ê²€ì¦ íƒœìŠ¤í¬
validate_results = PythonOperator(
    task_id='validate_scraping_results',
    python_callable=validate_scraping_results,
    dag=dag,
)

# ì™„ë£Œ ì•Œë¦¼ íƒœìŠ¤í¬
completion_notification = BashOperator(
    task_id='completion_notification',
    bash_command='''
    echo "=== ì˜¬ë¦¬ë¸Œì˜ ìƒí’ˆ í¬ë¡¤ë§ ì™„ë£Œ ==="
    echo "ì‹¤í–‰ ë‚ ì§œ: {{ ds }}"
    echo "ì™„ë£Œ ì‹œê°: $(date)"
    echo "ì²˜ë¦¬ëœ ì¹´í…Œê³ ë¦¬: {{ ti.xcom_pull(task_ids='validate_scraping_results')['total_categories'] }}ê°œ"
    echo "ì„±ê³µë¥ : {{ ti.xcom_pull(task_ids='validate_scraping_results')['success_rate'] }}%"
    echo "ì´ ìƒí’ˆ ìˆ˜: {{ ti.xcom_pull(task_ids='validate_scraping_results')['total_products_scraped'] }}ê°œ"
    echo "ë‹¤ìŒ ë‹¨ê³„: ë¦¬ë·° í¬ë¡¤ë§ DAG ì‹¤í–‰ ê°€ëŠ¥"
    ''',
    dag=dag,
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
# 1. ë¨¼ì € Cloud Run ì„œë¹„ìŠ¤ ë°°í¬
# 2. ë°°í¬ ì™„ë£Œ í›„ í¬ë¡¤ë§ ì‘ì—… ìƒì„±
# 3. ê° ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§ ë³‘ë ¬ ì‹¤í–‰
# 4. ê²°ê³¼ ê²€ì¦ ë° ì™„ë£Œ ì•Œë¦¼

deploy_service >> create_tasks >> scraping_tasks >> validate_results >> completion_notification

# ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•œ ì„¤ì •
for task in scraping_tasks:
    create_tasks >> task
